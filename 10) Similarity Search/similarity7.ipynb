{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784fcf81",
   "metadata": {},
   "source": [
    "### Custom Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59cd35a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 16:03:16.693383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750328296.713580 4086181 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750328296.719191 4086181 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750328296.735246 4086181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750328296.735275 4086181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750328296.735277 4086181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750328296.735279 4086181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-19 16:03:16.740349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e580c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load your data (example: movies, places, animals)\n",
    "data = {\n",
    "    \"movies\": [\"Inception\", \"The Matrix\", \"Interstellar\", \"Titanic\"],\n",
    "    \"places\": [\"Paris\", \"Tokyo\", \"New York\", \"Mount Everest\"],\n",
    "    \"animals\": [\"Tiger\", \"Elephant\", \"Dolphin\", \"Eagle\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim\n",
    "all_texts = data[\"movies\"] + data[\"places\"] + data[\"animals\"]\n",
    "embeddings = model.encode(all_texts).tolist()  # Convert to list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e67824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare metadata\n",
    "ids = [f\"item_{i}\" for i in range(len(all_texts))]\n",
    "metadatas = [{\"type\": \"movie\"} for _ in data[\"movies\"]] + \\\n",
    "            [{\"type\": \"place\"} for _ in data[\"places\"]] + \\\n",
    "            [{\"type\": \"animal\"} for _ in data[\"animals\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca376ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Store in Qdrant Cloud (free tier)\n",
    "client = QdrantClient(\n",
    "    url=\"https://2147d34e-a19b-4156-a7e8-7c8cb4b79b98.us-west-2-0.aws.cloud.qdrant.io:6333\", \n",
    "    api_key=qdrant_api_key,\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"game_data\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=\"game_data\",\n",
    "    points=[\n",
    "        {\n",
    "            \"id\": idx,\n",
    "            \"vector\": embedding,\n",
    "            \"payload\": {\"text\": text, \"type\": metadata[\"type\"]}\n",
    "        }\n",
    "        for idx, (embedding, text, metadata) in enumerate(zip(embeddings, all_texts, metadatas))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Embeddings uploaded to Qdrant Cloud!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9776aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_glove_file(file_path, lines_per_file=200000):\n",
    "    file_number = 1\n",
    "    line_count = 0\n",
    "    output = open(f\"{file_path}_part{file_number}.txt\", 'w', encoding='utf-8')\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            output.write(line)\n",
    "            line_count += 1\n",
    "\n",
    "            if line_count >= lines_per_file:\n",
    "                output.close()\n",
    "                file_number += 1\n",
    "                line_count = 0\n",
    "                output = open(f\"{file_path}_part{file_number}.txt\", 'w', encoding='utf-8')\n",
    "    \n",
    "    output.close()\n",
    "    print(f\"Finished splitting {file_path} into {file_number} parts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae23b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished splitting glove.6B.50d.txt into 3 parts.\n"
     ]
    }
   ],
   "source": [
    "split_glove_file('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644de6b",
   "metadata": {},
   "source": [
    "### GloVa Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get the data/embeddings\n",
    "\n",
    "# wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# unzip glove.6B.zip glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99763f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_glove_embeddings.py\n",
    "from qdrant_client import QdrantClient, models\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import time\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import mmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66309d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075319df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "QDRANT_URL = \"https://2147d34e-a19b-4156-a7e8-7c8cb4b79b98.us-west-2-0.aws.cloud.qdrant.io:6333\"  # From Qdrant Cloud dashboard\n",
    "api_key = os.getenv('qdrant_api_key')\n",
    "GLOVE_PATH = \"glove.6B.50d.txt\"  # 50-dim\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Qdrant\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=api_key,\n",
    "    timeout=120  # Increased timeout for large uploads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ff4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection (only run once!)\n",
    "client.recreate_collection(\n",
    "    collection_name=\"word_game_full\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=50,  # 50 dimensions for GloVe-50d\n",
    "        distance=models.Distance.COSINE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PARTS = [\n",
    "    \"glove.6B.50d_1.txt\",\n",
    "    \"glove.6B.50d_2.txt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd939f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split_glove():\n",
    "    word_count = 0\n",
    "    batch = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for glove_path in GLOVE_PARTS:\n",
    "        print(f\"Processing {glove_path}...\")\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.rstrip().split()\n",
    "                if len(parts) != 51:  # Skip malformed lines\n",
    "                    continue\n",
    "\n",
    "                word = parts[0]\n",
    "                vector = [float(x) for x in parts[1:51]]  # 50 dimensions\n",
    "\n",
    "                batch.append({\n",
    "                    \"id\": word_count,  # Using line number as ID\n",
    "                    \"vector\": vector,\n",
    "                    \"payload\": {\n",
    "                        \"word\": word,\n",
    "                        \"length\": len(word)\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                word_count += 1\n",
    "\n",
    "                if len(batch) >= BATCH_SIZE:\n",
    "                    client.upsert(\n",
    "                        collection_name=\"word_game_full\",\n",
    "                        points=batch,\n",
    "                        wait=True\n",
    "                    )\n",
    "                    batch = []\n",
    "                    print(f\"Processed {word_count} words | {word_count/(time.time()-start_time):.1f} words/sec\")\n",
    "\n",
    "        print(f\"Finished processing {glove_path}\")\n",
    "\n",
    "    if batch:\n",
    "        client.upsert(\n",
    "            collection_name=\"word_game_full\",\n",
    "            points=batch\n",
    "        )\n",
    "\n",
    "    print(f\"\\nFinished! Processed {word_count} total words\")\n",
    "    print(f\"Average speed: {word_count/(time.time()-start_time):.1f} words/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the processing\n",
    "process_full_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2621b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afb776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a37120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load your data (example: movies, places, animals)\n",
    "data = {\n",
    "    \"movies\": [\"Inception\", \"The Matrix\", \"Interstellar\", \"Titanic\"],\n",
    "    \"places\": [\"Paris\", \"Tokyo\", \"New York\", \"Mount Everest\"],\n",
    "    \"animals\": [\"Tiger\", \"Elephant\", \"Dolphin\", \"Eagle\"]\n",
    "}# 2. Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim\n",
    "all_texts = data[\"movies\"] + data[\"places\"] + data[\"animals\"]\n",
    "embeddings = model.encode(all_texts).tolist()  # Convert to list\n",
    "# 3. Prepare metadata\n",
    "ids = [f\"item_{i}\" for i in range(len(all_texts))]\n",
    "metadatas = [{\"type\": \"movie\"} for _ in data[\"movies\"]] + \\\n",
    "            [{\"type\": \"place\"} for _ in data[\"places\"]] + \\\n",
    "            [{\"type\": \"animal\"} for _ in data[\"animals\"]]\n",
    "# 4. Store in Qdrant Cloud (free tier)\n",
    "client = QdrantClient(\n",
    "    url=\"https://2147d34e-a19b-4156-a7e8-7c8cb4b79b98.us-west-2-0.aws.cloud.qdrant.io:6333\", \n",
    "    api_key=qdrant_api_key,\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"game_data\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=\"game_data\",\n",
    "    points=[\n",
    "        {\n",
    "            \"id\": idx,\n",
    "            \"vector\": embedding,\n",
    "            \"payload\": {\"text\": text, \"type\": metadata[\"type\"]}\n",
    "        }\n",
    "        for idx, (embedding, text, metadata) in enumerate(zip(embeddings, all_texts, metadatas))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Embeddings uploaded to Qdrant Cloud!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebadf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
