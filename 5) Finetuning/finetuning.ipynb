{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing evaluate lib\n",
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5528de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "# Define label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "# Generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"shawhin/imdb-truncated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edccde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer function\n",
    "def tokenize(examples):\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    # tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94502d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2155171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data collator\n",
    "# dynamically pad exaxmples in a given batch tobe as long as longes sequence\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# This is best if you have 400 lines in one batch but only 40 lines in other\n",
    "# so you dont have to waste resources by adding all those padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b59b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Defining an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, lables = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", # Sequence classification\n",
    "                        r=4, # intrinsic rank of trainable weigth matrix\n",
    "                        lora_alpha=32, # learning rate\n",
    "                        lora_dropout=0.01, # probablilty of dropout\n",
    "                        target_modules=['q_lin']) # applying lora to query layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237acecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837abf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # Add this to match save_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer Object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4943d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained model predictions/performance\n",
    "model.to('cpu')\n",
    "\n",
    "print(\"Trained Model Predictions: \")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
